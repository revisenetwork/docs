---
title: "Engineering Notes"
description: "Technical deep dives into debugging, distributed systems, and the architecture behind OnCall.  These are engineering notes documenting system behavior, architectural decisions, and real-world debugging lessons. Whether you use OnCall or not, we hope you learn something about how modern distributed systems break (and how to fix them)."
---

## Featured

<Card title="Why AI Debugging Needs a Hybrid Architecture: Local Context, Cloud Reasoning" icon="star" href="categories/architecture/why-ai-debugging-needs-hybrid-architecture">
  ![Why AI Debugging Needs a Hybrid Architecture: Local Context, Cloud Reasoning](/images/blog/cover.png)

  Why effective AI debugging requires local runtime introspection paired with cloud-based LLM reasoning.
</Card>

## Browse by categories


<CardGroup cols={2}>
  <Card title="Architecture" icon="newspaper" href="categories/architecture/index">
    Deep dives into local-first design, log ingestion, and distributed systems.
  </Card>
  <Card title="Code Intelligence" icon="newspaper" href="categories/code-intelligence/index">
    Static analysis, ASTs, and understanding code structure.
  </Card>
  <Card title="Debugging Cases" icon="newspaper" href="categories/debugging-cases/index">
    War stories, post-mortems, and real-world investigations.
  </Card>
  <Card title="Engineering Lessons" icon="newspaper" href="categories/engineering-lessons/index">
    Best practices, hard-learned truths, and dev culture.
  </Card>
  <Card title="Future of Debugging" icon="newspaper" href="categories/future-of-debugging/index">
    Agents, autonomous remediation, and the road ahead.
  </Card>
</CardGroup>

## Articles
import { CardGroup, Card } from "@/components/Card"
import { posts } from "@/content/index"

<CardGroup cols={2}>
  <Card
    title="Inside OnCall (Part 2): How Local Log Processing Supercharges Cloud AI Analysis"
    icon="newspaper"
    href="/categories/runtime-signals/inside-oncall-part-2-local-log-processing-runtime-signals"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    Why preprocessing logs locally creates cleaner signals and dramatically reduces LLM token usage.
  </Card>

  <Card
    title="Context Bloat vs Token Hunger: How to Balance LLM Inputs"
    icon="newspaper"
    href="/categories/llm-reasoning/context-bloat-vs-token-hunger-balancing-llm-inputs"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    How to balance context size and token limits in LLM systems without degrading reasoning quality or increasing cost.
  </Card>

  <Card
    title="The Hidden Cost of Context Bloat in AI Debugging—and How We Avoid It"
    icon="newspaper"
    href="/categories/llm-reasoning/hidden-cost-of-context-bloat-ai-debugging"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    Why excessive context hurts LLM debugging accuracy, and how minimal, signal-first context dramatically improves reasoning.
  </Card>

  <Card
    title="Inside OnCall (Part 4): How We Repurposed LangChain Deep Research"
    icon="newspaper"
    href="/categories/llm-reasoning/inside-oncall-part-4-repurposed-langchain-deep-research"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    How OnCall adapts LangChain’s Deep Research patterns to drive deeper, more reliable debugging-oriented LLM reasoning.
  </Card>

  <Card
    title="Inside OnCall (Part 3): Git-Aware Debugging and the Importance of Knowing What Changed"
    icon="newspaper"
    href="/categories/debugging-cases/inside-oncall-part-3-git-aware-debugging"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    How git diffs and time-travel file reads give LLMs the most powerful debugging context.
  </Card>

  <Card
    title="Inside OnCall (Part 1): How We Built Fast Local Code Intelligence"
    icon="newspaper"
    href="/categories/code-intelligence/inside-oncall-part-1-fast-local-code-intelligence"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    How OnCall uses fast, local code intelligence primitives to dramatically improve LLM-powered debugging and reasoning.
  </Card>

  <Card
    title="Why We Chose ripgrep Over grep for AI-Assisted Code Search"
    icon="newspaper"
    href="/categories/code-intelligence/ripgrep-vs-grep-ai-assisted-code-search"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    A practical, performance-driven comparison of ripgrep vs grep for AI-assisted code search workflows.
  </Card>

  <Card
    title="Why AI Debugging Needs a Hybrid Architecture: Local Context, Cloud Reasoning"
    icon="newspaper"
    href="/categories/architecture/why-ai-debugging-needs-hybrid-architecture"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    Why effective AI debugging requires local runtime introspection paired with cloud-based LLM reasoning.
  </Card>

  <Card
    title="Why AI Debugging Should Start Locally: The Case for On-Source Context Collection"
    icon="newspaper"
    href="/categories/engineering-lessons/why-ai-debugging-should-start-locally"
    img="/images/blog/cover.png"
    cta="Read more"
  >
    Why runtime-local context collection is essential for accurate, reliable AI-powered debugging.
  </Card>
</CardGroup>



## The Philosophy

<CardGroup cols={1}>
  <Card title="Why Logs > Prompts" icon="file-lines">
    **The Manifesto.** Copilots hallucinate because they lack context. OnCall bets on runtime signals over chat windows. Read why we process logs locally instead of relying on long prompts.
  </Card>
</CardGroup>

## Community & Events

<CardGroup cols={2}>
  <Card title="Beyond Prompts" icon="users">
    Recaps and recordings from our developer meetups in Bangalore.
  </Card>
  <Card title="OnCall Discord" icon="discord">
    Join the _Runtime Signals_ server to chat about distributed systems.
  </Card>
</CardGroup>

---

<Note>
  **Ready to debug faster?** OnCall reads your logs so you don't have to. [Get Early Access](https://oncall.build)
</Note>