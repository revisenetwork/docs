---
title: "Inside OnCall (Part 2): How Local Log Processing Supercharges Cloud AI Analysis"
date: "2025-12-10"
category: "Runtime Signals"
image: "/images/blog/cover.png"
description: "Why preprocessing logs locally creates cleaner signals and dramatically reduces LLM token usage."
---

# Inside OnCall Part 2: How Local Log Processing Supercharges Cloud AI Analysis

Created: December 4, 2025 2:16 PM
Article Summary (TLDR): Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis feugiat risus sed nunc accumsan, vitae ultrices urna accumsan. Integer orci purus, vestibulum vehicula dui mattis, mattis sollicitudin enim. Nulla facilisi. Praesent tempor eros nec tortor laoreet tempor. Suspendisse vitae lorem vel nisl malesuada placerat.
Author: Sunny (../Authors/Sunny%202bf13b3cead980fcb69cc584cfaf55ca.md)
Category: Debugging (../Categories/Debugging%202bf13b3cead9804283f6fe6c89ca154b.md)
Feature Article?: No
Publish Date: December 1, 2025

**Theme:** Runtime signals.

**Summary:**

Explain:

- Why raw logs → cloud = a bad idea
- How tail_logs, recent log chunks, and parse_stacktrace work
- How you normalize logs into structured context
- Why local pre-processing reduces token usage 10–50×
- How cloud AI uses these distilled signals

**Keywords:**

log preprocessing AI, AI error diagnosis, local log analysis, structured logs LLM, stacktrace parsing
